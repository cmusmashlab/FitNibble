{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eating-detection-exploration-pipeline.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Eating Detection Exploration Pipeline\n",
        "\n",
        "The code below executes the eating detection pipeline. There are three major sections\n",
        "\n",
        "- The Data Preparation pipeline is responsible for loading the data and spliting to different leave one person out data set. \n",
        "- We then benchmark different classical models\n",
        "- Finally, we benchmark some DNN approaches. "
      ],
      "metadata": {
        "id": "r0NrthFTrcjR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgv0QocPVNrb",
        "outputId": "7f5bb165-602a-492c-f02d-4cc74b2938b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive not mounted, so nothing to flush and unmount.\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import glob\n",
        "import os\n",
        "from packaging import version\n",
        "from time import sleep, strftime\n",
        "\n",
        "# Plotting\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from pandas.plotting import register_matplotlib_converters\n",
        "\n",
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import sklearn.model_selection\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import feature_column\n",
        "from tensorflow.keras import layers\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "ixl2_5InXjPD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae3119da-8ea6-4df4-87be-efecb1936bf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Dec 10 03:31:32 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The features directory has a file structured so every column represents features for 5 seconds window. The first row has the window number and the label of that window the rest of the 43 rows are features. These windows are consecutive in time with a time difference of 1 second between every adjacent windows (so it's a 5-sec sliding window sliding by 1 second at a time)\n",
        "\n",
        "5 Gyros, 5*3 = 15 channels + 1 proximity\n",
        "\n",
        "row values divided by 16, each ordered as below:\n",
        "\n",
        "Median\n",
        "Entropy\n",
        "Variance\n",
        "RMS\n",
        "zero cross - Variance :\n",
        "zero crossing\n",
        "Select the below features from the gyroscopes & proximi values [7,8,9,10,11,12,16] ==> [7,8,9] --> G1, [10,11,12] --> G2 and 16 --> prox\n",
        "\n",
        "6 * 7 channels == xg1,yg1,zg1,xg2,yg2,zg2,Proximity.\n",
        "\n",
        "File: 5_slid_1_featuerslabels_DStest2.csv\n",
        "\n",
        "Look for all the activity folder(5) in segmented data, and in each activity leave on participant out.\n",
        "\n",
        "Labels: remove None, change label with _'s to only the first values."
      ],
      "metadata": {
        "id": "XtOQ-BE3ZEHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "activities=['snack','meeting','TV','hike','gym']\n",
        "classes=['silent','eating','drinking','talking','walking']\n",
        "features_list=['Median','Entropy','Variance','RMS','ZeroCrossingVar',\n",
        "               'ZeroCrossing']\n",
        "all_sensor_list = ['g1','g2','g3','g4','g5','prox']\n",
        "sub_sensor_list = ['g3','g4','prox']\n",
        "sensors={'g1':np.array([0,1,2]),'g2':np.array([3,4,5]),'g3':np.array([6,7,8]),'g4':np.array([9,10,11]),'g5':np.array([12,13,14]),'prox':np.array([15])}\n",
        "\n",
        "\n",
        "# Creates {'Median': 1, 'Entropy': 17, 'Variance': 33, 'RMS': 49, 'ZeroCrossingVar': 65, 'ZeroCrossing': 81}\n",
        "index=0\n",
        "feature_index={key: 0 for key in features_list}\n",
        "for feature in features_list:\n",
        "    count= 16\n",
        "    # feature_index[feature]=index+1 #for numpy array\n",
        "    feature_index[feature]=index # for Pandas\n",
        "    index+=count\n",
        "    \n",
        "participants_all = []\n",
        "participants_include = []\n",
        "participant_exclude = []\n",
        "\n",
        "\n",
        "# Paths!\n",
        "data_directory = 'dataset/analysis/data/SegmentedData'\n",
        "lopo_data_directory = 'dataset/samples/lopo_data/'\n",
        "features_directory = 'dataset/data_features/'\n",
        "models_directory = 'dataset/models/'\n",
        "models_directory_lopo = 'dataset/lopo-models/'\n",
        "results_directory_lopo = 'dataset/results_lopo/'\n",
        "\n",
        "\n",
        "# # Dataframes\n",
        "global entire_data, entire_test_data, entire_train_data\n",
        "entire_data = pd.DataFrame()\n",
        "entire_test_data = pd.DataFrame()\n",
        "entire_train_data = pd.DataFrame()\n"
      ],
      "metadata": {
        "id": "Vv0KHDunXnS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexToSelect = []\n",
        "for sensor in sub_sensor_list:\n",
        "  for feature in features_list:\n",
        "    # print(list(sensors[sensor]+feature_index[feature]))\n",
        "    for itm in list(sensors[sensor]+feature_index[feature]):\n",
        "      indexToSelect.append(itm)\n",
        "indexToSelect.sort()\n",
        "print (indexToSelect)"
      ],
      "metadata": {
        "id": "N3uuo04cZ8AL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre-Preparation"
      ],
      "metadata": {
        "id": "WUpsAV03Z9U1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_features_file_for_all_activities(csv_file, path, new_file_name):\n",
        "  global entire_data\n",
        "  csv_data = pd.read_csv(path+'/'+csv_file)\n",
        "  # print (csv_data.head())\n",
        "  filtered_csv_data = csv_data.filter(indexToSelect,axis=0).reset_index(drop=True)\n",
        "  print(filtered_csv_data.shape, csv_data.shape, csv_data.index)\n",
        "  # print(filtered_csv_data.head(),'\\n',filtered_csv_data.tail())\n",
        "  filtered_csv_data.to_csv(features_directory+new_file_name,index=False)\n",
        "\n",
        "  filtered_csv_data.columns = filtered_csv_data.columns.str.split(': ').str[1]\n",
        "  filtered_csv_data = filtered_csv_data.T\n",
        "  print(filtered_csv_data.shape)\n",
        "\n",
        "  if entire_data.empty:\n",
        "    print('DataFrame is empty!')\n",
        "    entire_data = filtered_csv_data.copy(deep=True)\n",
        "  else:\n",
        "    entire_data = entire_data.append(filtered_csv_data,ignore_index=True)\n",
        "\n",
        "def clear_entire_data():\n",
        "  global entire_data\n",
        "  del(entire_data)\n",
        "  entire_data = pd.DataFrame()"
      ],
      "metadata": {
        "id": "R0q8KHMwaDVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_lopo_test_data_features(csv_file, path, new_file_name):\n",
        "  global entire_test_data\n",
        "  csv_data = pd.read_csv(path+'/'+csv_file)\n",
        "  filtered_csv_data = csv_data.filter(indexToSelect,axis=0).reset_index(drop=True)\n",
        "  print(filtered_csv_data.shape, csv_data.shape, csv_data.index)\n",
        "  filtered_csv_data.columns = filtered_csv_data.columns.str.split(': ').str[1]\n",
        "  filtered_csv_data = filtered_csv_data.T\n",
        "  print(filtered_csv_data.shape)\n",
        "\n",
        "  if entire_test_data.empty:\n",
        "    print('DataFrame is empty!')\n",
        "    entire_test_data = filtered_csv_data.copy(deep=True)\n",
        "  else:\n",
        "    entire_test_data = entire_test_data.append(filtered_csv_data)\n",
        "\n",
        "\n",
        "def create_lopo_train_data_features(csv_file, path, new_file_name):\n",
        "  global entire_train_data\n",
        "  csv_data = pd.read_csv(path+'/'+csv_file)\n",
        "  filtered_csv_data = csv_data.filter(indexToSelect,axis=0).reset_index(drop=True)\n",
        "  print(filtered_csv_data.shape, csv_data.shape, csv_data.index)\n",
        "  filtered_csv_data.columns = filtered_csv_data.columns.str.split(': ').str[1]\n",
        "  filtered_csv_data = filtered_csv_data.T\n",
        "\n",
        "  if entire_train_data.empty:\n",
        "    print('DataFrame is empty!')\n",
        "    entire_train_data = filtered_csv_data.copy(deep=True)\n",
        "  else:\n",
        "    entire_train_data = entire_train_data.append(filtered_csv_data)\n",
        "  \n",
        "def clear_entire_data_lopo():\n",
        "  global entire_train_data, entire_test_data\n",
        "  del(entire_train_data)\n",
        "  del(entire_test_data)\n",
        "\n",
        "  entire_train_data = pd.DataFrame()\n",
        "  entire_test_data = pd.DataFrame()"
      ],
      "metadata": {
        "id": "cUEipc7GaQhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_data_to_five_classes():\n",
        "  global entire_train_data\n",
        "  global entire_test_data\n",
        "  entire_train_data = entire_train_data.drop('none')\n",
        "  entire_train_data = entire_train_data.rename(index={'drinking_walking': 'drinking'})\n",
        "  entire_train_data = entire_train_data.rename(index={'walking_talking': 'walking'})\n",
        "  entire_train_data = entire_train_data.rename(index={'drinking_talking': 'drinking'})\n",
        "  entire_train_data = entire_train_data.rename(index={'eating_talking': 'eating'})\n",
        "  entire_train_data = entire_train_data.rename(index={'eating_walking': 'eating'})\n",
        "  entire_train_data = entire_train_data.rename(index={'eating_walking_talking': 'eating'})\n",
        "  entire_train_data = entire_train_data.rename(index={'drinking_walking_talking': 'drinking'})\n",
        "  entire_train_data = entire_train_data.rename(index={'eating_drinking': 'eating'})\n",
        "  entire_train_data = entire_train_data.rename(index={'eating_drinking_talking': 'eating'})\n",
        "\n",
        "  entire_test_data = entire_test_data.drop('none')\n",
        "  entire_test_data = entire_test_data.rename(index={'drinking_walking': 'drinking'})\n",
        "  entire_test_data = entire_test_data.rename(index={'walking_talking': 'walking'})\n",
        "  entire_test_data = entire_test_data.rename(index={'drinking_talking': 'drinking'})\n",
        "  entire_test_data = entire_test_data.rename(index={'eating_talking': 'eating'})\n",
        "  entire_test_data = entire_test_data.rename(index={'eating_walking': 'eating'})\n",
        "  entire_test_data = entire_test_data.rename(index={'eating_walking_talking': 'eating'})\n",
        "  entire_test_data = entire_test_data.rename(index={'drinking_walking_talking': 'drinking'})\n",
        "  entire_test_data = entire_test_data.rename(index={'eating_drinking': 'eating'})\n",
        "  entire_test_data = entire_test_data.rename(index={'eating_drinking_talking': 'eating'})\n",
        "  print(\"updating data to five classes\")"
      ],
      "metadata": {
        "id": "pWzJe38CaeQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code loops throught the dataset and creates a Leave-One-Out dataset."
      ],
      "metadata": {
        "id": "eL5ebcSPaked"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ext = \"*.csv\"\n",
        "all_csv_files =[]\n",
        "\n",
        "if not entire_train_data.empty:\n",
        "    clear_entire_data_lopo()\n",
        "\n",
        "# Find all participants\n",
        "for path, subdir, files in os.walk(data_directory):\n",
        "    # print (path, subdir) \n",
        "    for file in files:\n",
        "       if file =='5_slid_1_featuerslabels_DStest2.csv':\n",
        "         participant_name = str(path.split(os.path.sep)[-1])\n",
        "         if participant_name not in participants_all:\n",
        "           participants_all.append(participant_name)\n",
        "          #  print(participants_all)\n",
        "\n",
        "print(participants_all)\n",
        "\n",
        "flag =0\n",
        "# Create LOPO/LOOCV dataset for the participants. \n",
        "for participant in participants_all:\n",
        "  \n",
        "  lopo_path = lopo_data_directory + 'leave_participant_' + participant \n",
        "  \n",
        "  print('Leaving Path', participant)\n",
        "  print('Leaving Path', lopo_path)\n",
        "\n",
        "  if not os.path.exists(lopo_path):\n",
        "    # Create a new directory because it does not exist \n",
        "    os.makedirs(lopo_path)\n",
        "    print(\"The new directory is created!\",lopo_path)\n",
        "\n",
        "  for path, subdir, files in os.walk(data_directory):\n",
        "      print (path, subdir) \n",
        "      for file in files:\n",
        "        if file =='5_slid_1_featuerslabels_DStest2.csv':\n",
        "          participant_folder = str(path.split(os.path.sep)[-1])\n",
        "          new_file_name = str(path.split(os.path.sep)[-2])+'_'+ \\\n",
        "                str(path.split(os.path.sep)[-1])+\\\n",
        "                '_filtered_5_slid_1_featuerslabels_DStest2.csv'\n",
        "          print(new_file_name)\n",
        "          # testing:\n",
        "          if participant == participant_folder:\n",
        "            create_lopo_test_data_features(file, path, new_file_name)\n",
        "          else:\n",
        "            create_lopo_train_data_features(file, path, new_file_name)\n",
        "\n",
        "  update_data_to_five_classes()\n",
        "\n",
        "  print('\\n\\n\\n********** Final shape entire_train_data')\n",
        "  print(entire_train_data.shape, entire_train_data.index)\n",
        "  print('saving ' + lopo_path+'/train_data.csv' )\n",
        "  # save test and train data\n",
        "  entire_train_data.to_csv(lopo_path+'/train_data.csv')\n",
        "\n",
        "  print('\\n\\n\\n********** Final shape entire_test_data')\n",
        "  print(entire_test_data.shape, entire_test_data.index)\n",
        "  print('saving ' + lopo_path+'/test_data.csv' )\n",
        "  entire_test_data.to_csv(lopo_path+'/test_data.csv')\n",
        "  clear_entire_data_lopo()\n",
        "  # break;"
      ],
      "metadata": {
        "id": "V5BJs7_ZakI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Data"
      ],
      "metadata": {
        "id": "16N-FmKqa41f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "participant = ''\n",
        "data_directory_train = lopo_data_directory+'leave_participant_'+participant+'/train_data.csv'\n",
        "data_train = pd.read_csv(data_directory_train,index_col=0)\n",
        "data_train.head()\n",
        "\n",
        "sns.countplot(x = entire_train_data.index,\n",
        "              data = entire_train_data,\n",
        "              order = entire_train_data.index.value_counts().index);"
      ],
      "metadata": {
        "id": "wwsPxV6qa8sF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_directory_test = '/content/drive/MyDrive/TFLite-Exploration-Eating/Samples/lopo_data/leave_participant_S/test_data.csv'\n",
        "entire_data_t = pd.read_csv(data_directory_test,index_col=0)\n",
        "entire_data_t.shape"
      ],
      "metadata": {
        "id": "2JkChNCDa_Yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Classical models"
      ],
      "metadata": {
        "id": "C9N2rJocbrqG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Leave one out data logging with prediction logging."
      ],
      "metadata": {
        "id": "LTFczYTtcMOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "participants_all = []\n",
        "# Training Basic Models.\n",
        "models_list = [knn_model, svm_rbf_model, svc_model, dtc_model, rnf_model,\n",
        "               mlp_model, ada_model, gnb_model]\n",
        "\n",
        "\n",
        "results_file_csv = results_directory_lopo + 'results_lopo.csv'\n",
        "results = []\n",
        "\n",
        "if not os.path.exists(results_directory_lopo):\n",
        "  print(\"make-dir\", results_directory_lopo)\n",
        "  os.makedirs(results_directory_lopo)\n",
        "               \n",
        "for participant in participants_all:\n",
        "  print('leave_participant_',participant)\n",
        "\n",
        "  data_directory_train = lopo_data_directory+'leave_participant_'+participant+'/train_data.csv'\n",
        "  data_directory_test = lopo_data_directory+'leave_participant_'+participant+'/test_data.csv'\n",
        "  \n",
        "  print('reading',data_directory_train)\n",
        "  print('reading',data_directory_test)\n",
        "\n",
        "  data_train = pd.read_csv(data_directory_train,index_col=0)\n",
        "  data_test = pd.read_csv(data_directory_test,index_col=0)\n",
        "\n",
        "  X_train = data_train.to_numpy()\n",
        "  y_train = data_train.index.to_numpy()\n",
        "  X_test = data_test.to_numpy()\n",
        "  y_test = data_test.index.to_numpy()\n",
        "\n",
        "  ##########################\n",
        "  #######Training\n",
        "  ##########################\n",
        "  print('\\n\\n###################')\n",
        "  print(\"All Training Started\", 'leave_participant_', participant)\n",
        "  print('###################\\n')\n",
        "  for model in models_list:\n",
        "    print(\"Training Started\", 'leave_participant_', participant , model)\n",
        "    start = time.time()\n",
        "    print(model)\n",
        "    model.fit(X_train, y_train)\n",
        "    end = time.time()\n",
        "    print(\"Time taken to train: \", str(end - start))\n",
        "\n",
        "    print(\"Storing the Models\")\n",
        "\n",
        "    model_name = type(model).__name__\n",
        "    location = models_directory_lopo+ model_name+ '/'\n",
        "    if not os.path.exists(location):\n",
        "      print(\"make-dir\", location)\n",
        "      os.makedirs(location)\n",
        "    location_filename = location + 'leave_participant_'+participant+'_'+model_name +\".pickle\" \n",
        "    pickle.dump(model, open(location_filename, 'wb'))\n",
        "    # print(\"Done Storing the Models\")\n",
        "    print(\"Training Ended\", 'leave_participant_', participant , model)\n",
        "  print(\"All Training Done\", 'leave_participant_', participant)\n",
        "  \n",
        "  ##########################\n",
        "  #######Prediction\n",
        "  ##########################\n",
        "  print('\\n###################')\n",
        "  print(\"All Prediction Started\", 'leave_participant_', participant)\n",
        "  print('###################\\n')\n",
        "  for model in models_list:\n",
        "    print(\"Prediction Started\", 'leave_participant_', participant , type(model).__name__)\n",
        "    start = time.time()\n",
        "    print(model)\n",
        "    y_pred = model.predict(X_test)\n",
        "    end = time.time()\n",
        "    prediction_time = str(end - start)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(\"Time taken to predict: \", prediction_time)\n",
        "    print(\"Accuracy: \", accuracy)      \n",
        "    with open(results_file_csv, \"a\") as log:\n",
        "      log.write(\"{0},{1},leave_participant_{2},{3},{4}\\n\".format(strftime(\"%Y-%m-%d %H:%M:%S\"),type(model).__name__,participant,str(accuracy),str(prediction_time)))\n",
        "    print('wrote_to_results')\n",
        "    results.append([type(model).__name__,participant,str(accuracy),str(prediction_time)])\n",
        "    print([[x,y_pred.tolist().count(x)] for x in set(y_pred.tolist())])\n",
        "    print(\"Prediction Done\", 'leave_participant_', participant)\n",
        "    print(\"\\n\\n\")\n",
        "  print(\"All Prediction Ended\", 'leave_participant_', participant)"
      ],
      "metadata": {
        "id": "XNNaqFnZb0SC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning Model Explorations"
      ],
      "metadata": {
        "id": "cL7Xi3pacurt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1 : Custom DNN"
      ],
      "metadata": {
        "id": "DS0mwfieeR9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train TF model - 1 Definition\n",
        "\n",
        "# Build the model and train it\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.Dense(50, activation='relu')) # relu is used for performance\n",
        "model.add(tf.keras.layers.Dense(15, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')) # softmax is used, because we only expect one activity to occur per input\n",
        "\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=4e-4)\n"
      ],
      "metadata": {
        "id": "eXJyR3k0fd_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for participant in participants_all:\n",
        "  print('leave_participant_',participant)\n",
        "\n",
        "  data_directory_train = lopo_data_directory+'leave_participant_'+participant+'/train_data.csv'\n",
        "  data_directory_test = lopo_data_directory+'leave_participant_'+participant+'/test_data.csv'\n",
        "  \n",
        "  print('reading',data_directory_train)\n",
        "  print('reading',data_directory_test)\n",
        "\n",
        "  data_train = pd.read_csv(data_directory_train,index_col=0)\n",
        "  data_test = pd.read_csv(data_directory_test,index_col=0)\n",
        "\n",
        "  X_train = data_train.to_numpy()\n",
        "  y_train = data_train.index.to_numpy()\n",
        "  X_test = data_test.to_numpy()\n",
        "  y_test = data_test.index.to_numpy()\n",
        "\n",
        "  enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "\n",
        "  y_train_enc = np.array(y_train).reshape(-1, 1)\n",
        "  y_test_enc = np.array(y_test).reshape(-1, 1) \n",
        "  y_val_enc = np.array(y_test_val).reshape(-1, 1) \n",
        "\n",
        "  enc = enc.fit(y_train_enc)\n",
        "\n",
        "  y_train_enc = enc.transform(y_train_enc)\n",
        "  y_test_enc = enc.transform(y_test_enc)\n",
        "  y_val_enc = enc.transform(y_val_enc)\n",
        "\n",
        "  print(\"Training Started\", 'leave_participant_', participant , model)\n",
        "\n",
        "  model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  history = model.fit(X_train, y_train_enc, validation_data=(X_test_val,y_val_enc), epochs=250, batch_size=1)\n",
        "\n",
        "  plt.plot(history.history['loss'], label='train')\n",
        "  plt.plot(history.history['val_loss'], label='test')\n",
        "  plt.legend();\n",
        "  \n",
        "  location=models_directory+ 'five-classes-new-type-input-all-data/' + 'TF-model-1' + '/'\n",
        "  if not os.path.exists(location):\n",
        "    print(\"make-dir\", location)\n",
        "    os.makedirs(location)\n",
        "    model.save(location+'leave_participant_'+participant+'_custom')\n",
        "    if not os.path.exists(location):\n",
        "      print(\"make-dir\", location)\n",
        "      os.makedirs(location)\n",
        "      # print(\"Done Storing the Models\")\n",
        "    print(\"Training Ended\", 'leave_participant_', participant , model)"
      ],
      "metadata": {
        "id": "eqIiUj5feQ85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 2 - TF"
      ],
      "metadata": {
        "id": "Ne6oKBQcc0c0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_2 = tf.keras.Sequential([\n",
        "            layers.Dense(128, activation='relu'),\n",
        "            layers.Dense(128, activation='relu'),\n",
        "            layers.Dropout(.1),\n",
        "            layers.Dense(1)])\n"
      ],
      "metadata": {
        "id": "CKJUYQPkczFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " for participant in participants_all:\n",
        "  print('leave_participant_',participant)\n",
        "\n",
        "  data_directory_train = lopo_data_directory+'leave_participant_'+participant+'/train_data.csv'\n",
        "  data_directory_test = lopo_data_directory+'leave_participant_'+participant+'/test_data.csv'\n",
        "  \n",
        "  print('reading',data_directory_train)\n",
        "  print('reading',data_directory_test)\n",
        "\n",
        "  data_train = pd.read_csv(data_directory_train,index_col=0)\n",
        "  data_test = pd.read_csv(data_directory_test,index_col=0)\n",
        "\n",
        "  X_train = data_train.to_numpy()\n",
        "  y_train = data_train.index.to_numpy()\n",
        "  X_test = data_test.to_numpy()\n",
        "  y_test = data_test.index.to_numpy()\n",
        "\n",
        "  enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "\n",
        "  y_train_enc = np.array(y_train).reshape(-1, 1)\n",
        "  y_test_enc = np.array(y_test).reshape(-1, 1) \n",
        "  y_val_enc = np.array(y_test_val).reshape(-1, 1) \n",
        "\n",
        "  enc = enc.fit(y_train_enc)\n",
        "\n",
        "  y_train_enc = enc.transform(y_train_enc)\n",
        "  y_test_enc = enc.transform(y_test_enc)\n",
        "  y_val_enc = enc.transform(y_val_enc)\n",
        "\n",
        "  print(\"Training Started\", 'leave_participant_', participant , model)\n",
        "  \n",
        "  model_2.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "  \n",
        "  history = model_2.fit(X_train, y_train_enc, validation_data=(X_test_val,y_val_enc), epochs=250, batch_size=1)\n",
        "\n",
        "\n",
        "  model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  history = model.fit(X_train, y_train_enc, validation_data=(X_test_val,y_val_enc), epochs=250, batch_size=1)\n",
        "\n",
        "  plt.plot(history.history['loss'], label='train')\n",
        "  plt.plot(history.history['val_loss'], label='test')\n",
        "  plt.legend();\n",
        "  \n",
        "  location=models_directory+ 'five-classes-new-type-input-all-data/' + 'TF-model-2' + '/'\n",
        "  if not os.path.exists(location):\n",
        "    print(\"make-dir\", location)\n",
        "    os.makedirs(location)\n",
        "    model.save(location+'leave_participant_'+participant+'_custom')\n",
        "    if not os.path.exists(location):\n",
        "      print(\"make-dir\", location)\n",
        "      os.makedirs(location)\n",
        "      # print(\"Done Storing the Models\")\n",
        "    print(\"Training Ended\", 'leave_participant_', participant , model)"
      ],
      "metadata": {
        "id": "KG7tG37fergj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 3 - GBT"
      ],
      "metadata": {
        "id": "AfN4VzPaicrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install TensorFlow Dececision Forests.\n",
        "!pip install tensorflow_decision_forests\n",
        "\n",
        "# Use wurlitzer to capture training logs.\n",
        "!pip install wurlitzer"
      ],
      "metadata": {
        "id": "7LH6eFZiicRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_decision_forests as tfdf\n",
        "\n",
        "try:\n",
        "  from wurlitzer import sys_pipes\n",
        "except:\n",
        "  from colabtools.googlelog import CaptureLog as sys_pipes\n",
        "\n",
        "from IPython.core.magic import register_line_magic\n",
        "from IPython.display import Javascript\n",
        "\n",
        "# Check the version of TensorFlow Decision Forests\n",
        "print(\"Found TensorFlow Decision Forests v\" + tfdf.__version__)"
      ],
      "metadata": {
        "id": "w1tY2ycQevx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "participants_all = []\n",
        "\n",
        "results_file_csv = results_directory_lopo + 'results_lopo.csv'\n",
        "results = []\n",
        "\n",
        "if not os.path.exists(results_directory_lopo):\n",
        "  print(\"make-dir\", results_directory_lopo)\n",
        "  os.makedirs(results_directory_lopo)\n",
        "               \n",
        "for participant in participants_all:\n",
        "  print('leave_participant_',participant)\n",
        "\n",
        "  data_directory_train = lopo_data_directory+'leave_participant_'+participant+'/train_data.csv'\n",
        "  data_directory_test = lopo_data_directory+'leave_participant_'+participant+'/test_data.csv'\n",
        "  \n",
        "  print('reading',data_directory_train)\n",
        "  print('reading',data_directory_test)\n",
        "\n",
        "  data_train = pd.read_csv(data_directory_train,index_col=0)\n",
        "  data_test = pd.read_csv(data_directory_test,index_col=0)\n",
        "  \n",
        "  X_train = data_train.to_numpy()\n",
        "  y_train = data_train.index.to_numpy()\n",
        "  X_test = data_test.to_numpy()\n",
        "  y_test = data_test.index.to_numpy()\n",
        "\n",
        "  print(X_train.shape)\n",
        "  print(X_test.shape)\n",
        "  print(y_train.shape)\n",
        "  print(y_test.shape)\n",
        "  break;\n",
        "\n",
        "  train_ds_pd = data_train.copy(deep=True)\n",
        "  train_ds_pd['label'] = data_train.index\n",
        "  train_ds_pd['label'].unique()\n",
        "  test_ds_pd = data_test.copy(deep=True)\n",
        "  test_ds_pd['label'] = data_test.index\n",
        "\n",
        "  train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label='label')\n",
        "  test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_ds_pd, label='label')\n",
        "\n",
        "  for feature_batch, label_batch in test_ds.take(1):\n",
        "    print('Every feature:', list(feature_batch.keys()))\n",
        "    print('A batch of inputs:', feature_batch)\n",
        "    print('A batch of targets:', label_batch )\n",
        "  type(test_ds)\n",
        "\n",
        "  ds = test_ds.take(1)  # Only take a single example\n",
        "\n",
        "  for example in ds:  # example is `{'image': tf.Tensor, 'label': tf.Tensor}`\n",
        "    print(list(example[0])\n",
        "    image = example[\"image\"]\n",
        "    label = example[\"label\"]\n",
        "    print(image.shape, label)\n",
        "  \n",
        "  model_1 = tfdf.keras.GradientBoostedTreesModel(\n",
        "    num_trees=500,\n",
        "    growing_strategy=\"BEST_FIRST_GLOBAL\",\n",
        "    max_depth=8,\n",
        "    split_axis=\"SPARSE_OBLIQUE\",\n",
        "    categorical_algorithm=\"RANDOM\",\n",
        "    )\n",
        "\n",
        "  # Optionally, add evaluation metrics.\n",
        "  model_1.compile(\n",
        "      metrics=[\"accuracy\"])\n",
        "\n",
        "  # Train the model.\n",
        "  # \"sys_pipes\" is optional. It enables the display of the training logs.\n",
        "  with sys_pipes():\n",
        "    model_1.fit(x=train_ds)\n",
        "  \n",
        "  evaluation = model_1.evaluate(test_ds, return_dict=True)\n",
        "\n",
        "  location=models_directory+ 'five-classes-new-type-input-all-data/' + 'GBT-TF-model' + '/'\n",
        "  if not os.path.exists(location):\n",
        "    print(\"make-dir\", location)\n",
        "    os.makedirs(location)\n",
        "    model.save(location+'leave_participant_'+participant+'_custom')\n",
        "    if not os.path.exists(location):\n",
        "      print(\"make-dir\", location)\n",
        "      os.makedirs(location)\n",
        "      # print(\"Done Storing the Models\")\n",
        "    print(\"Training Ended\", 'leave_participant_', participant , model)"
      ],
      "metadata": {
        "id": "vAEkavRjjhb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 4 -  DNN Model "
      ],
      "metadata": {
        "id": "-8p5BLN5lTZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_m = Sequential()\n",
        "model_m.add(Reshape((TIME_PERIODS, 3), input_shape=(input_shape,)))\n",
        "model_m.add(Dense(100, activation='relu'))\n",
        "model_m.add(Dense(100, activation='relu'))\n",
        "model_m.add(Dense(100, activation='relu'))\n",
        "model_m.add(Flatten())\n",
        "model_m.add(Dense(num_classes, activation='softmax'))\n",
        "print(model_m.summary())"
      ],
      "metadata": {
        "id": "8fR1uOUejomY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " for participant in participants_all:\n",
        "  print('leave_participant_',participant)\n",
        "\n",
        "  data_directory_train = lopo_data_directory+'leave_participant_'+participant+'/train_data.csv'\n",
        "  data_directory_test = lopo_data_directory+'leave_participant_'+participant+'/test_data.csv'\n",
        "  \n",
        "  print('reading',data_directory_train)\n",
        "  print('reading',data_directory_test)\n",
        "\n",
        "  data_train = pd.read_csv(data_directory_train,index_col=0)\n",
        "  data_test = pd.read_csv(data_directory_test,index_col=0)\n",
        "\n",
        "  X_train = data_train.to_numpy()\n",
        "  y_train = data_train.index.to_numpy()\n",
        "  X_test = data_test.to_numpy()\n",
        "  y_test = data_test.index.to_numpy()\n",
        "\n",
        "  enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "\n",
        "  y_train_enc = np.array(y_train).reshape(-1, 1)\n",
        "  y_test_enc = np.array(y_test).reshape(-1, 1) \n",
        "  y_val_enc = np.array(y_test_val).reshape(-1, 1) \n",
        "\n",
        "  enc = enc.fit(y_train_enc)\n",
        "\n",
        "  y_train_enc = enc.transform(y_train_enc)\n",
        "  y_test_enc = enc.transform(y_test_enc)\n",
        "  y_val_enc = enc.transform(y_val_enc)\n",
        "\n",
        "  print(\"Training Started\", 'leave_participant_', participant , model)\n",
        "  \n",
        "  model_2.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "  \n",
        "  history = model_2.fit(X_train, y_train_enc, validation_data=(X_test_val,y_val_enc), epochs=250, batch_size=1)\n",
        "\n",
        "\n",
        "  model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  history = model.fit(X_train, y_train_enc, validation_data=(X_test_val,y_val_enc), epochs=250, batch_size=1)\n",
        "\n",
        "  plt.plot(history.history['loss'], label='train')\n",
        "  plt.plot(history.history['val_loss'], label='test')\n",
        "  plt.legend();\n",
        "  \n",
        "  location=models_directory+ 'five-classes-new-type-input-all-data/' + 'TF-model-4' + '/'\n",
        "  if not os.path.exists(location):\n",
        "    print(\"make-dir\", location)\n",
        "    os.makedirs(location)\n",
        "    model.save(location+'leave_participant_'+participant+'_custom')\n",
        "    if not os.path.exists(location):\n",
        "      print(\"make-dir\", location)\n",
        "      os.makedirs(location)\n",
        "      # print(\"Done Storing the Models\")\n",
        "    print(\"Training Ended\", 'leave_participant_', participant , model)"
      ],
      "metadata": {
        "id": "Ga3d9d-ZlQKJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}